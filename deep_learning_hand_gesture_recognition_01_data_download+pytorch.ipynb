{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_hand_gesture_recognition_01_data_download.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doyeon16/test/blob/master/deep_learning_hand_gesture_recognition_01_data_download%2Bpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVkmxbz4wJRX",
        "colab_type": "text"
      },
      "source": [
        "# Hand Gesture Datasets\n",
        "\n",
        "This notebook/colab downloads hand gesture datasets and stores them as pickle files.\n",
        "\n",
        "For more information, please take a look at: https://github.com/guillaumephd/deep_learning_hand_gesture_recognition/issues/1\n",
        "\n",
        "Note: a (very minor) bug present in the third step described in that issue, when calling `train_test_split` function, is corrected in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1co5vDJYlwcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 1. Download hand gesture datasets\n",
        "# ---------------------------------------------------------\n",
        "download_shrec_17 = False\n",
        "download_dhg = True\n",
        "download_online_dhg = False\n",
        "\n",
        "# --------------------------\n",
        "# SHREC2017 dataset\n",
        "#     http://www-rech.telecom-lille.fr/shrec2017-hand/\n",
        "# --------------------------\n",
        "if download_shrec_17:\n",
        "  !mkdir dataset_shrec2017\n",
        "  !wget http://www-rech.telecom-lille.fr/shrec2017-hand/HandGestureDataset_SHREC2017.tar.gz -O SHREC2017.tar.gz\n",
        "  !tar -xzf SHREC2017.tar.gz -C dataset_shrec2017\n",
        "# --------------------------\n",
        "# DHG14/28 dataset\n",
        "#     http://www-rech.telecom-lille.fr/DHGdataset/\n",
        "# --------------------------\n",
        "# Note: you should register on http://www-rech.telecom-lille.fr/DHGdataset/ before downloading the dataset\n",
        "if download_dhg:\n",
        "  !mkdir dataset_dhg1428\n",
        "  !wget http://www-rech.telecom-lille.fr/DHGdataset/DHG2016.zip\n",
        "  !unzip DHG2016.zip -d dataset_dhg1428\n",
        "# --------------------------\n",
        "# Online DHG dataset\n",
        "#     http://www-rech.telecom-lille.fr/shrec2017-hand/\n",
        "# --------------------------\n",
        "if download_online_dhg:\n",
        "  !mkdir dataset_onlinedhg\n",
        "  !wget http://www-rech.telecom-lille.fr/shrec2017-hand/OnlineDHG.zip\n",
        "  !unzip OnlineDHG.zip -d dataset_onlinedhg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N5i0LEcxRtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 2. Utils\n",
        "# ---------------------------------------------------------\n",
        "import glob\n",
        "import numpy\n",
        "import pickle\n",
        "from scipy import ndimage as ndimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def resize_gestures(input_gestures, final_length=100):\n",
        "    \"\"\"\n",
        "    Resize the time series by interpolating them to the same length\n",
        "\n",
        "    Input:\n",
        "        - input_gestures: list of numpy.ndarray tensors.\n",
        "              Each tensor represents a single gesture.\n",
        "              Gestures can have variable durations.\n",
        "              Each tensor has a shape: (duration, channels)\n",
        "              where duration is the duration of the individual gesture\n",
        "                    channels = 44 = 2 * 22 if recorded in 2D and\n",
        "                    channels = 66 = 3 * 22 if recorded in 3D \n",
        "    Output:\n",
        "        - output_gestures: one numpy.ndarray tensor.\n",
        "              The output tensor has a shape: (records, final_length, channels)\n",
        "              where records = len(input_gestures)\n",
        "                   final_length is the common duration of all gestures\n",
        "                   channels is the same as above \n",
        "    \"\"\"\n",
        "    # please use python3. if you still use python2, important note: redefine the classic division operator / by importing it from the __future__ module\n",
        "    output_gestures = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in input_gestures])\n",
        "    return output_gestures\n",
        "\n",
        "\n",
        "def load_gestures(dataset='dhg', root='/content/dataset_dhg1428', version_x='3D', version_y='both', resize_gesture_to_length=100):\n",
        "    \"\"\"\n",
        "    Get the 3D or 2D pose gestures sequences, and their associated labels.\n",
        "\n",
        "    Ouput:\n",
        "        - a tuple of (gestures, labels) or (gestures, labels_14, labels_28)\n",
        "              where gestures is either a numpy.ndarray tensor or\n",
        "                                       a list of numpy.ndarray tensors,\n",
        "                                       depending on if the gestures have been resized or not.\n",
        "              Each tensor represents a single gesture.\n",
        "              Gestures can have variable durations.\n",
        "              Each tensor has a shape: (duration, channels) where channels is either 44 (= 2 * 22) or 66 (=3 * 22)\n",
        "    \"\"\"\n",
        "\n",
        "    # SHREC 2017 (on Google Colab):\n",
        "    # root = '/content/dataset_shrec2017/HandGestureDataset_SHREC2017'\n",
        "    # DHG 14/28 (on Google Colab):\n",
        "    # root = '/content/dataset_dhg1428'\n",
        "    if dataset == 'dhg':\n",
        "      assert 'dataset_dhg' in root\n",
        "    if dataset == 'shrec':\n",
        "      assert 'dataset_shrec' in root\n",
        "    \n",
        "    if version_x == '3D':\n",
        "        if dataset == 'dhg':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeleton_world.txt'\n",
        "        elif dataset == 'shrec':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeletons_world.txt'\n",
        "    else:\n",
        "        if dataset == 'dhg':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeleton_image.txt'\n",
        "        elif dataset == 'shrec':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeletons_image.txt'\n",
        "\n",
        "    gestures_filenames = sorted(glob.glob(pattern))\n",
        "    gestures = [numpy.genfromtxt(f) for f in gestures_filenames]\n",
        "    if resize_gesture_to_length is not None:\n",
        "        gestures = resize_gestures(gestures, final_length=resize_gesture_to_length)\n",
        "\n",
        "    labels_14 = [int(filename.split('/')[-5].split('_')[1]) for filename in gestures_filenames]\n",
        "    labels_28 = [int(filename.split('/')[-4].split('_')[1]) for filename in gestures_filenames]\n",
        "    labels_28 = [labels_14[idx_gesture] if n_fingers_used == 1 else 14 + labels_14[idx_gesture] for idx_gesture, n_fingers_used in enumerate(labels_28)]\n",
        "\n",
        "    if version_y == '14' or version_y == 14:\n",
        "        return gestures, labels_14\n",
        "    elif version_y == '28' or version_y == 28:\n",
        "        return gestures, labels_28\n",
        "    elif version_y == 'both':\n",
        "        return gestures, labels_14, labels_28\n",
        "\n",
        "\n",
        "def write_data(data, filepath):\n",
        "    \"\"\"Save the dataset to a file. Note: data is a dict with keys 'x_train', ...\"\"\"\n",
        "    with open(filepath, 'wb') as output_file:\n",
        "        pickle.dump(data, output_file)\n",
        "\n",
        "\n",
        "def load_data(filepath='./shrec_data.pckl'):\n",
        "    \"\"\"\n",
        "    Returns hand gesture sequences (X) and their associated labels (Y).\n",
        "    Each sequence has two different labels.\n",
        "    The first label  Y describes the gesture class out of 14 possible gestures (e.g. swiping your hand to the right).\n",
        "    The second label Y describes the gesture class out of 28 possible gestures (e.g. swiping your hand to the right with your index pointed, or not pointed).\n",
        "    \"\"\"\n",
        "    file = open(filepath, 'rb')\n",
        "    data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
        "    file.close()\n",
        "    return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZBMcQtxSkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 3. Save the dataset(s) you need\n",
        "# ---------------------------------------------------------\n",
        "# Example: 3D version of the SHREC17 and DHG gesture datasets, with gestures resized to 100 timesteps\n",
        "gestures, labels_14, labels_28 = load_gestures(dataset='dhg',\n",
        "                                               root='/content/dataset_dhg1428',\n",
        "                                               version_x='3D',\n",
        "                                               version_y='both',\n",
        "                                               resize_gesture_to_length=100)\n",
        "# Split the dataset into train and test sets if you want:\n",
        "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = train_test_split(gestures, labels_14, labels_28, test_size=0.30)\n",
        "\n",
        "# Save the dataset\n",
        "data = {\n",
        "    'x_train': x_train,\n",
        "    'x_test': x_test,\n",
        "    'y_train_14': y_train_14,\n",
        "    'y_train_28': y_train_28,\n",
        "    'y_test_14': y_test_14,\n",
        "    'y_test_28': y_test_28\n",
        "}\n",
        "write_data(data, filepath='dhg_data.pckl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfM0cws7jhsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4de3e8f1-b6df-4bd0-e341-d73f52f96135"
      },
      "source": [
        "print(gestures.shape)\n",
        "print(len(labels_14))\n",
        "print(len(labels_28))\n",
        "print(type(x_train))\n",
        "print(type(y_train_14))\n",
        "print(type(y_train_28))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(len(y_train_14))\n",
        "#print(y_test_14.shape)\n",
        "print(len(y_train_28))\n",
        "#print(y_test_28.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2800, 100, 66)\n",
            "2800\n",
            "2800\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "(1960, 100, 66)\n",
            "(840, 100, 66)\n",
            "1960\n",
            "840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWuNFbKGyFbn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f7e64f5-104e-4974-e64b-b69d9c8f2417"
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 4. Optional: copy to google drive, if you're in a Google Colab\n",
        "# ---------------------------------------------------------\n",
        "try:\n",
        "\n",
        "  # Connect Google Colab instance to Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "\n",
        "  # Save your dataset on Google Drive\n",
        "  !cp dhg_data.pckl /gdrive/My\\ Drive/dhg_data.pckl\n",
        "\n",
        "  # Load your dataset from Google Drive\n",
        "  # !cp /gdrive/My\\ Drive/dhg_data.pckl dhg_data.pckl\n",
        "\n",
        "except:\n",
        "  print(\"You're not in a Google Colab!\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDKQ_UL6yF75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 5. Use the dataset(s)\n",
        "# ---------------------------------------------------------\n",
        "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data('dhg_data.pckl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2n2Gts7MP51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d95dc2dd-8b18-4978-d628-9bb8ede7fb2a"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(len(y_train_14))\n",
        "#print(y_test_14.shape)\n",
        "print(len(y_train_28))\n",
        "#print(y_test_28.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1960, 100, 66)\n",
            "(840, 100, 66)\n",
            "1960\n",
            "840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mAA0XWbEmDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import sys\n",
        "if sys.version_info.major < 3:\n",
        "    print('You are using python 2, but you should rather use python 3.')\n",
        "    print('    If you still want to use python 2, ensure you import:')\n",
        "    print('    >> from __future__ import unicode_literals, print_function, division')\n",
        "\n",
        "import numpy\n",
        "import pickle\n",
        "import torch\n",
        "import itertools\n",
        "import time\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj9_6CDuEnVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (bonus) plot acc with tensorboard\n",
        "#   Command to start tensorboard if installed (requires tensorflow):\n",
        "#   $  tensorboard --logdir ./runs\n",
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "except:\n",
        "    # tensorboardX is not installed, just fail silently\n",
        "    class SummaryWriter():\n",
        "        def __init__(self):\n",
        "            pass\n",
        "        def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5FoZTKKEqIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 14\n",
        "duration = 100\n",
        "n_channels = 66\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf8vgQSBEsMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create model\n",
        "\n",
        "\n",
        "class HandGestureNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    [Devineau et al., 2018] Deep Learning for Hand Gesture Recognition on Skeletal Data\n",
        "\n",
        "    Summary\n",
        "    -------\n",
        "        Deep Learning Model for Hand Gesture classification using pose data only (no need for RGBD)\n",
        "        The model computes a succession of [convolutions and pooling] over time independently on each of the 66 (= 22 * 3) sequence channels.\n",
        "        Each of these computations are actually done at two different resolutions, that are later merged by concatenation\n",
        "        with the (pooled) original sequence channel.\n",
        "        Finally, a multi-layer perceptron merges all of the processed channels and outputs a classification.\n",
        "    \n",
        "    TL;DR:\n",
        "    ------\n",
        "        input ------------------------------------------------> split into n_channels channels [channel_i]\n",
        "            channel_i ----------------------------------------> 3x [conv/pool/dropout] low_resolution_i\n",
        "            channel_i ----------------------------------------> 3x [conv/pool/dropout] high_resolution_i\n",
        "            channel_i ----------------------------------------> pooled_i\n",
        "            low_resolution_i, high_resolution_i, pooled_i ----> output_channel_i\n",
        "        MLP(n_channels x [output_channel_i]) -------------------------> classification\n",
        "\n",
        "    Article / PDF:\n",
        "    --------------\n",
        "        https://ieeexplore.ieee.org/document/8373818\n",
        "\n",
        "    Please cite:\n",
        "    ------------\n",
        "        @inproceedings{devineau2018deep,\n",
        "            title={Deep learning for hand gesture recognition on skeletal data},\n",
        "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
        "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
        "            pages={106--113},\n",
        "            year={2018},\n",
        "            organization={IEEE}\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_channels=66, n_classes=14, dropout_probability=0.2):\n",
        "\n",
        "        super(HandGestureNet, self).__init__()\n",
        "        \n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_probability = dropout_probability\n",
        "\n",
        "        # Layers ----------------------------------------------\n",
        "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=self.dropout_probability),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=self.dropout_probability),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.AvgPool1d(2),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
        "        )\n",
        "\n",
        "        # Initialization --------------------------------------\n",
        "        # Xavier init\n",
        "        for module in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
        "            for layer in module:\n",
        "                if layer.__class__.__name__ == \"Conv1d\":\n",
        "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "        for layer in self.fc:\n",
        "            if layer.__class__.__name__ == \"Linear\":\n",
        "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                torch.nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        This function performs the actual computations of the network for a forward pass.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "            input: a tensor of gestures of shape (batch_size, duration, n_channels)\n",
        "                   (where n_channels = 3 * n_joints for 3D pose data)\n",
        "        \"\"\"\n",
        "\n",
        "        # Work on each channel separately\n",
        "        all_features = []\n",
        "\n",
        "        for channel in range(0, self.n_channels):\n",
        "            input_channel = input[:, :, channel]\n",
        "\n",
        "            # Add a dummy (spatial) dimension for the time convolutions\n",
        "            # Conv1D format : (batch_size, n_feature_maps, duration)\n",
        "            input_channel = input_channel.unsqueeze(1)\n",
        "\n",
        "            high = self.all_conv_high[channel](input_channel)\n",
        "            low = self.all_conv_low[channel](input_channel)\n",
        "            ap_residual = self.all_residual[channel](input_channel)\n",
        "\n",
        "            # Time convolutions are concatenated along the feature maps axis\n",
        "            output_channel = torch.cat([\n",
        "                high,\n",
        "                low,\n",
        "                ap_residual\n",
        "            ], dim=1)\n",
        "            all_features.append(output_channel)\n",
        "\n",
        "        # Concatenate along the feature maps axis\n",
        "        all_features = torch.cat(all_features, dim=1)\n",
        "        \n",
        "        # Flatten for the Linear layers\n",
        "        all_features = all_features.view(-1, 9 * self.n_channels * 12)  # <-- 12: depends of the initial sequence length (100).\n",
        "        # If you have shorter/longer sequences, you probably do NOT even need to modify the modify the network architecture:\n",
        "        # resampling your input gesture from T timesteps to 100 timesteps will (surprisingly) probably actually work as well!\n",
        "\n",
        "        # Fully-Connected Layers\n",
        "        output = self.fc(all_features)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh7wAlOmEyqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------\n",
        "# Network instantiation\n",
        "# -------------\n",
        "model = HandGestureNet(n_channels=n_channels, n_classes=n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfpg64e2E0pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a77fb2df-e264-40d8-ae49-0463cf957f47"
      },
      "source": [
        "#data load\n",
        "# We load a gesture dataset:\n",
        "#\n",
        "#   x.shape should be (dataset_size, duration, channel)\n",
        "#   y.shape should be (dataset_size, 1)\n",
        "\n",
        "\n",
        "# If you want to use the DHG dataset, go to: https://colab.research.google.com/drive/1ggYG1XRpJ50gVgJqT_uoI257bspNogHj\n",
        "use_dhg_dataset = True\n",
        "\n",
        "if use_dhg_dataset:\n",
        "    # ------------------------\n",
        "    # DHG Dataset\n",
        "    # ------------------------\n",
        "    try:\n",
        "        # Connect Google Colab instance to Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/gdrive')\n",
        "        # Load the dataset (you already have created in the other notebook) from Google Drive\n",
        "        !cp /gdrive/My\\ Drive/dhg_data.pckl dhg_data.pckl\n",
        "    except:\n",
        "        print(\"You're not in a Google Colab!\")\n",
        "\n",
        "    def load_data(filepath='./shrec_data.pckl'):\n",
        "        \"\"\"\n",
        "        Returns hand gesture sequences (X) and their associated labels (Y).\n",
        "        Each sequence has two different labels.\n",
        "        The first label  Y describes the gesture class out of 14 possible gestures (e.g. swiping your hand to the right).\n",
        "        The second label Y describes the gesture class out of 28 possible gestures (e.g. swiping your hand to the right with your index pointed, or not pointed).\n",
        "        \"\"\"\n",
        "        file = open(filepath, 'rb')\n",
        "        data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
        "        file.close()\n",
        "        return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']\n",
        "\n",
        "    x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data('dhg_data.pckl')\n",
        "    y_train_14, y_test_14 = numpy.array(y_train_14), numpy.array(y_test_14)\n",
        "    y_train_28, y_test_28 = numpy.array(y_train_28), numpy.array(y_test_28)\n",
        "    if n_classes == 14:\n",
        "        y_train = y_train_14\n",
        "        y_test = y_test_14\n",
        "    elif n_classes == 28:\n",
        "        y_train = y_train_28\n",
        "        y_test = y_test_28\n",
        "\n",
        "else:\n",
        "    # ------------------------\n",
        "    # Custom Dataset\n",
        "    # ------------------------\n",
        "    # On the left bar of this colaboratory notebook there is a section called \"Files\".\n",
        "    # Upload your files there and use a path like \"/content/each_file_you_just_uploaded\" to load your data\n",
        "    # \n",
        "    # For now, for the sake of demonstration purposes, let's create fake data\n",
        "    x_train = numpy.random.randn(2000, duration, n_channels)\n",
        "    y_train = numpy.random.random_integers(n_classes, size=2000)\n",
        "\n",
        "    x_test = numpy.random.randn(1000, duration, n_channels)\n",
        "    y_test = numpy.random.random_integers(n_classes, size=1000)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAJJtvqXE6uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GestureDataset(Dataset):\n",
        " \n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8jOaPXwE-Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------\n",
        "# Create pytorch datasets and dataloaders:\n",
        "# ------------------------\n",
        "# Convert from numpy to torch format\n",
        "x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
        "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
        "\n",
        "# Ensure the label values are between 0 and n_classes-1\n",
        "if y_train.min() > 0:\n",
        "  y_train = y_train - 1\n",
        "if y_test.min() > 0:\n",
        "  y_test = y_test - 1\n",
        "\n",
        "# Ensure the data type is correct\n",
        "x_train, x_test = x_train.float(), x_test.float()\n",
        "y_train, y_test = y_train.long(), y_test.long()\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = GestureDataset(x=x_train, y=y_train)\n",
        "test_dataset = GestureDataset(x=x_test, y=y_test)\n",
        "\n",
        "# Pytorch dataloaders are used to group dataset items into batches\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size=32, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrwEaKN-FAp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def time_since(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '{:02d}m {:02d}s'.format(int(m), int(s))\n",
        "\n",
        "\n",
        "def get_accuracy(model, x, y_ref):\n",
        "    \"\"\"Get the accuracy of the pytorch model on a batch\"\"\"\n",
        "    acc = 0.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predicted = model(x)\n",
        "        _, predicted = predicted.max(dim=1)\n",
        "        acc = 1.0 * (predicted == y_ref).sum().item() / y_ref.shape[0]\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3DSEL7MFDBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training model\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Loss function & Optimizer\n",
        "# -----------------------------------------------------\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jOXRygcFHDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------\n",
        "# Training\n",
        "# -------------\n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, dataloader,\n",
        "          x_train, y_train, x_test, y_test,\n",
        "          force_cpu=False, num_epochs=5):\n",
        "    \n",
        "    # use a GPU (for speed) if you have one\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() and not force_cpu else torch.device(\"cpu\")\n",
        "    model = model#.to(device)\n",
        "    #x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "    #x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "\n",
        "    # (bonus) log accuracy values to visualize them in tensorboard:\n",
        "    writer = SummaryWriter()\n",
        "    \n",
        "    # Training starting time\n",
        "    start = time.time()\n",
        "\n",
        "    print('[INFO] Started to train the model.')\n",
        "    print('Training the model on {}.'.format('GPU' if device == torch.device('cuda') else 'CPU'))\n",
        "    \n",
        "    for ep in range(num_epochs):\n",
        "\n",
        "        # Ensure we're still in training mode\n",
        "        model.train()\n",
        "\n",
        "        current_loss = 0.0\n",
        "\n",
        "        for idx_batch, batch in enumerate(dataloader):\n",
        "\n",
        "            # Move data to GPU, if available\n",
        "            x, y = batch\n",
        "            #x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # zero the gradient parameters\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            y_pred = model(x)\n",
        "\n",
        "            # backward + optimize\n",
        "            # backward\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            # optimize\n",
        "            optimizer.step()\n",
        "            # for an easy access\n",
        "            current_loss += loss.item()\n",
        "        \n",
        "        train_acc = get_accuracy(model, x_train, y_train)\n",
        "        test_acc = get_accuracy(model, x_test, y_test)\n",
        "        \n",
        "        writer.add_scalar('data/accuracy_train', train_acc, ep)\n",
        "        writer.add_scalar('data/accuracy_test', test_acc, ep)\n",
        "        print('Epoch #{:03d} | Time elapsed : {} | Loss : {:.4e} | Accuracy_train : {:.2f}% | Accuracy_test : {:.2f}% '.format(\n",
        "                ep + 1, time_since(start), current_loss, 100 * train_acc, 100 * test_acc))\n",
        "\n",
        "    print('[INFO] Finished training the model. Total time : {}.'.format(time_since(start)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl-8ovL_mk7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "bb0690c1-9bbc-4cf3-a829-6b146c78de57"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(type(y_train))\n",
        "print(len(y_train))\n",
        "print(type(y_test))\n",
        "print(len(y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1960, 100, 66])\n",
            "torch.Size([840, 100, 66])\n",
            "<class 'torch.Tensor'>\n",
            "1960\n",
            "<class 'torch.Tensor'>\n",
            "1960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8w4NlDzFKsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "741e3174-9f60-4b5f-873b-7bf1c62a30ad"
      },
      "source": [
        "# Please adjust the training epochs count, and the other hyperparams (lr, dropout, ...), for a non-overfitted training according to your own needs.\n",
        "# tip: use tensorboard to display the accuracy (see cells above for tensorboard usage)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "train(model=model, criterion=criterion, optimizer=optimizer, dataloader=train_dataloader,\n",
        "      x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, num_epochs=num_epochs)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Started to train the model.\n",
            "Training the model on CPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-58f1a509278c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train(model=model, criterion=criterion, optimizer=optimizer, dataloader=train_dataloader,\n\u001b[0;32m----> 5\u001b[0;31m       x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, num_epochs=num_epochs)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-53d9ac08214d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, dataloader, x_train, y_train, x_test, y_test, force_cpu, num_epochs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/accuracy_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-537217e65093>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, x, y_ref)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (840) must match the size of tensor b (1960) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXk3-WSFMwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get trained model\n",
        "\n",
        "# Reminder: first redefine/load the HandGestureNet class before you use it, if you want to use it elsewhere\n",
        "model = HandGestureNet(n_channels=n_channels, n_classes=n_classes)\n",
        "model.load_state_dict(torch.load('gesture_pretrained_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "# make predictions\n",
        "with torch.no_grad():\n",
        "    demo_gesture_batch = torch.randn(32, duration, n_channels)\n",
        "    predictions = model(demo_gesture_batch)\n",
        "    _, predictions = predictions.max(dim=1)\n",
        "    print(\"Predicted gesture classes: {}\".format(predictions.tolist()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}